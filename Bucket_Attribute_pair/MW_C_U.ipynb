{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from ftfy import fix_text\n",
    "# from util import UnitConversion, mapping_list_values, perl_to_posix\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 500\n",
    "from flashtext import KeywordProcessor\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import ast\n",
    "import warnings\n",
    "import regex as re\n",
    "warnings.filterwarnings('ignore')\n",
    "from decimal import *\n",
    "TWOPLACES = Decimal(10) ** -2\n",
    "from natsort import natsorted\n",
    "import ast\n",
    "import time\n",
    "today = time.strftime(\"%Y_%m_%d\")\n",
    "from enrich_dimensions.rounds import rounds, rounding_inch_feet,rounding_lbs,rounding_w,rounding_oz, rounding_lb,rounding_gal, re_extract, curate, round_string_float, clean_data,reg_ex,rounding_period_after_unit \n",
    "from enrich_dimensions.params import parameters, query_from_file\n",
    "from enrich_dimensions.query_file import query_from_file \n",
    "from enrich_dimensions.custom import custom_field \n",
    "\n",
    "\n",
    "def get_unique_list(material_list):\n",
    "    unique_list = []\n",
    "    for item in material_list:\n",
    "        if isinstance(item, str) and \"[\" in item and \"]\" in item:\n",
    "            item_values = ast.literal_eval(item)\n",
    "            unique_list.extend(item_values)\n",
    "        else:\n",
    "            unique_list.append(item)\n",
    "    return sorted(list(set(unique_list)))\n",
    "\n",
    "\n",
    "\n",
    "customer_id = '21'\n",
    "customer_name='%menswearhouse%'\n",
    "client='Mens Warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Query of Curated Data\n",
      "35773\n"
     ]
    }
   ],
   "source": [
    "dateszs='2001-08-11'\n",
    "\n",
    "dateszs='2001-11-01'\n",
    "params = {'customer_id': customer_id,\n",
    "          'customer_name':customer_name,\n",
    "         'dateszs':dateszs}\n",
    "\n",
    "print('Start Query of Curated Data')\n",
    "cb = query_from_file(file_name='query/curated_all_attributes_date_family.sql', params=params)\n",
    "print(len(cb))\n",
    "\n",
    "fill_dict = cb.groupby('external_id')['buckets'].last().to_dict()\n",
    "cb['buckets'] = cb['buckets'].fillna(cb['external_id'].map(fill_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Attribute bucket Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Number of attributes: 33\n",
      "\n",
      "Start Buckets\n",
      "0 Attribute: belt_width\n",
      "Number of SKUs for the attribute belt_width: 33\n",
      "Start Buckets\n",
      "1 Attribute: care\n",
      "Number of SKUs for the attribute care: 3269\n",
      "Start Buckets\n",
      "2 Attribute: category\n",
      "Number of SKUs for the attribute category: 3834\n",
      "Start Buckets\n",
      "3 Attribute: collar_style\n",
      "Number of SKUs for the attribute collar_style: 579\n",
      "Start Buckets\n",
      "4 Attribute: color\n",
      "Number of SKUs for the attribute color: 3791\n",
      "Start Buckets\n",
      "5 Attribute: cuff_style\n",
      "Number of SKUs for the attribute cuff_style: 417\n",
      "Start Buckets\n",
      "6 Attribute: cufflink_shape\n",
      "Number of SKUs for the attribute cufflink_shape: 21\n",
      "Start Buckets\n",
      "7 Attribute: features\n",
      "Number of SKUs for the attribute features: 3666\n",
      "Buckets Effected: 2\n",
      "Number of SKUs to be wiped: 9\n",
      "\n",
      "\n",
      "Start Buckets\n",
      "8 Attribute: fit\n",
      "Number of SKUs for the attribute fit: 1433\n",
      "Start Buckets\n",
      "9 Attribute: jacket_style\n",
      "Number of SKUs for the attribute jacket_style: 528\n",
      "Start Buckets\n",
      "10 Attribute: lapel_shape\n",
      "Number of SKUs for the attribute lapel_shape: 2\n",
      "Start Buckets\n",
      "11 Attribute: lapel_style\n",
      "Number of SKUs for the attribute lapel_style: 553\n",
      "Start Buckets\n",
      "12 Attribute: length\n",
      "Number of SKUs for the attribute length: 0\n",
      "Start Buckets\n",
      "13 Attribute: lining\n",
      "Number of SKUs for the attribute lining: 156\n",
      "Start Buckets\n",
      "14 Attribute: material\n",
      "Number of SKUs for the attribute material: 3446\n",
      "Start Buckets\n",
      "15 Attribute: metal_tone\n",
      "Number of SKUs for the attribute metal_tone: 171\n",
      "Start Buckets\n",
      "16 Attribute: neckline\n",
      "Number of SKUs for the attribute neckline: 279\n",
      "Start Buckets\n",
      "17 Attribute: occasion\n",
      "Number of SKUs for the attribute occasion: 1072\n",
      "Start Buckets\n",
      "18 Attribute: package_quantity\n",
      "Number of SKUs for the attribute package_quantity: 356\n",
      "Start Buckets\n",
      "19 Attribute: pant_style\n",
      "Number of SKUs for the attribute pant_style: 352\n",
      "Start Buckets\n",
      "20 Attribute: pattern\n",
      "Number of SKUs for the attribute pattern: 3689\n",
      "Start Buckets\n",
      "21 Attribute: product_type\n",
      "Number of SKUs for the attribute product_type: 3838\n",
      "Buckets Effected: 6\n",
      "Number of SKUs to be wiped: 8\n",
      "\n",
      "\n",
      "Start Buckets\n",
      "22 Attribute: scent\n",
      "Number of SKUs for the attribute scent: 15\n",
      "Start Buckets\n",
      "23 Attribute: sleeve_length\n",
      "Number of SKUs for the attribute sleeve_length: 795\n",
      "Start Buckets\n",
      "24 Attribute: sock_style\n",
      "Number of SKUs for the attribute sock_style: 244\n",
      "Start Buckets\n",
      "25 Attribute: sole_material\n",
      "Number of SKUs for the attribute sole_material: 202\n",
      "Start Buckets\n",
      "26 Attribute: suspender_style\n",
      "Number of SKUs for the attribute suspender_style: 10\n",
      "Start Buckets\n",
      "27 Attribute: tie_width\n",
      "Number of SKUs for the attribute tie_width: 761\n",
      "Start Buckets\n",
      "28 Attribute: toe_shape\n",
      "Number of SKUs for the attribute toe_shape: 271\n",
      "Start Buckets\n",
      "29 Attribute: toe_style\n",
      "Number of SKUs for the attribute toe_style: 266\n",
      "Start Buckets\n",
      "30 Attribute: upper_material\n",
      "Number of SKUs for the attribute upper_material: 266\n",
      "Start Buckets\n",
      "31 Attribute: waist\n",
      "Number of SKUs for the attribute waist: 14\n",
      "Start Buckets\n",
      "32 Attribute: wash\n",
      "Number of SKUs for the attribute wash: 16\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())))\n",
    "lst_attribute=sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())\n",
    "print('Number of attributes: '+str(len(lst_attribute)))\n",
    "print('')\n",
    "\n",
    "# lst_attribute=['color']\n",
    "for i in range(len(lst_attribute)):\n",
    "    attri=lst_attribute[i]\n",
    "    dateszs='2001-08-11'\n",
    "    attribut=attri\n",
    "    dateszs='2001-11-01'\n",
    "    curation_col = f'Q:{attribut}'\n",
    "    params = {'customer_id': customer_id,\n",
    "              'customer_name':customer_name,\n",
    "             'dateszs':dateszs,\n",
    "             'attribut':attribut}\n",
    "\n",
    "    print('Start Buckets')\n",
    "    print(str(i)+' Attribute: '+str(attri))\n",
    "    bucket_value = query_from_file(file_name='query/Bucket_Value_Strategy.sql', params=params)\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "#     print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "    print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    filtered_dfs = {}\n",
    "    b_lst=[]\n",
    "\n",
    "    for i in range(len(list_of_all_buckets)):\n",
    "\n",
    "        # values that are supposed to be in the buckets\n",
    "        action_bucket_values=bucket_value[bucket_value['buckets'].astype(str)==list_of_all_buckets[i]]['values'].to_list()\n",
    "\n",
    "        # values that are actually in the curation\n",
    "        action=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]['value'].to_list()\n",
    "\n",
    "        unique_values=get_unique_list(action)\n",
    "\n",
    "        target=[x for x in unique_values if x not in action_bucket_values]\n",
    "        if len(target) > 0:\n",
    "            target = target[0]\n",
    "            pat=f'''(?i)({target})|()'''\n",
    "            bucket_filtered_df=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]\n",
    "            bucket_filtered_df['match']=bucket_filtered_df['value'].apply(lambda x: re_extract(pat,str(x)))\n",
    "            fil=bucket_filtered_df[bucket_filtered_df['match'].astype(str)!='[]']\n",
    "            filtered_dfs[list_of_all_buckets[i]] = fil\n",
    "            b_lst.append(list_of_all_buckets[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    b_lst[0:10]\n",
    "    valuez = {}\n",
    "    external_id_list=[]\n",
    "    for i in range(len(b_lst)):\n",
    "        valuez[b_lst[i]]=filtered_dfs[b_lst[i]][['external_id','value']]\n",
    "        external_id_list.append(filtered_dfs[b_lst[i]]['external_id'].tolist())\n",
    "    import itertools\n",
    "    flattened_list = list(itertools.chain.from_iterable(external_id_list))\n",
    "    if len(flattened_list)>0:\n",
    "        print('Buckets Effected: '+str(len(b_lst)))\n",
    "        df_name = f'match_{attri}'\n",
    "        data = {'external_id': flattened_list, f'Q:{attri}': '[]'}\n",
    "        df_dict = {df_name: pd.DataFrame(data)}\n",
    "        new_df = df_dict[df_name]\n",
    "        print('Number of SKUs to be wiped: '+str(len(new_df)))\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "        def get_df_name(df):\n",
    "            name =[x for x in globals() if globals()[x] is df][0]\n",
    "            return name\n",
    "        def looks_good(customer, matches,attri): \n",
    "            drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "            matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attri}-{today}.csv',index=False) \n",
    "        looks_good(client, new_df,attri) \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Freetext Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = {\n",
    "#     'appliance_cubic_feet': r'(cu ft)|(n\\/a)|()',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_matches(dataframe, attribute, pattern):\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re_extract(pattern,str(x)))\n",
    "    empty_matches = matches.apply(lambda x: len(x) == 0)\n",
    "    return dataframe[dataframe['attribute'].astype(str) == attribute][empty_matches]\n",
    "\n",
    "def count_empty_matches(dataframe, attribute, pattern):\n",
    "    print(f'Number of SKUs for {attribute}: '+str(len(dataframe[dataframe['attribute'].astype(str) == attribute])))\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re_extract(pattern,str(x)))\n",
    "    return sum(len(match) == 0 for match in matches)\n",
    "\n",
    "\n",
    "filtered_dataframes = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    filtered_dataframes[attribute] = filter_empty_matches(cb, attribute, pattern)\n",
    "    \n",
    "counts = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    counts[attribute] = count_empty_matches(cb, attribute, pattern)\n",
    "#     print(f'Rows that need to be wiped for {attribute}: '+f': {counts[attribute]}')\n",
    "   \n",
    "\n",
    "    try:\n",
    "        flattened_list = filtered_dataframes[attribute]['external_id'].to_list()\n",
    "        if len(flattened_list) > 0:\n",
    "            df_name = f'match_{attribute}'\n",
    "            data = {'external_id': flattened_list, f'Q:{attribute}': '[]'}\n",
    "            df_dict = {df_name: pd.DataFrame(data)}\n",
    "            new_df = df_dict[df_name]\n",
    "            print('Number of SKUs to be wiped: ' + str(len(new_df)))\n",
    "            print('')\n",
    "            print('')\n",
    "\n",
    "            def get_df_name(df):\n",
    "                name = [x for x in globals() if globals()[x] is df][0]\n",
    "                return name\n",
    "\n",
    "            def looks_good(customer, matches, attribute):\n",
    "                today = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "                drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload'\n",
    "                matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attribute}-{today}.csv', index=False)\n",
    "            looks_good(client, new_df, attribute)\n",
    "            \n",
    "        else:\n",
    "            print('No SKUs to be wiped for ' + attribute)\n",
    "    except KeyError:\n",
    "        print('Error: DataFrame with name ' + df_name + ' does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit=cb[cb['attribute'].astype(str)=='fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n"
     ]
    }
   ],
   "source": [
    "pat='''(?i)((?:athletic.?fit|classic.?fit|extreme.?slim.?fit|executive.?fit|extreme.?slim|husky|modern.?fit|portly|regular.?fit|relaxed.?fit|skinny.?fit|slim.?fit|straight.?fit))|()'''                                \n",
    "\n",
    "fit['m_name']=fit['name'].apply(lambda x: re_extract(pat,str(x)))\n",
    "fit['m_long']=fit['long_desc'].apply(lambda x: re_extract(pat,str(x)))\n",
    "fit['m_custom']=fit['custom_fields'].apply(lambda x: re_extract(pat,str(x)))\n",
    "\n",
    "name=fit[fit['m_name'].astype(str)!='[]']\n",
    "print(len(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-e4697b676722>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-e4697b676722>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    exteme slim fit not for : - Sports shirt button-downs - camp shirt button-downs\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "exteme slim fit not for : - Sports shirt button-downs - camp shirt button-downs\n",
    "    \n",
    "    \n",
    "husky for: applicable for Boys: - Dress Suits- Coats/Blazers- Dress Pants\n",
    "If title/description does not call out a specific fit, curate as \"Regular Fit\".\n",
    "\n",
    "\n",
    "modern fit: Include Regular Fit\n",
    "    \n",
    "    \n",
    "Portly only applies to - Blazers/Suit Coats- Business/Formal Dress Suits- Dress shirts button-downs\n",
    "\n",
    "\n",
    "Regular fit: For BOYS items that do not say any specific Fit on the title/description. \n",
    "    \n",
    "    \n",
    "Slim Fit: NOT applicable for Camp Shirt button downs Does NOT include items that call out \"trim Modern fit\" -- only tag Modern Fit for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>value</th>\n",
       "      <th>m_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>TMW_6NJD_6NJE_01</td>\n",
       "      <td>[\"Classic Fit\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>TMW_8V9G_8VRG_02</td>\n",
       "      <td>Classic Fit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>TMW_8V9G_8VRG_19</td>\n",
       "      <td>Classic Fit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>TMW_8V9H_8VRH_19</td>\n",
       "      <td>Classic Fit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>TMW_6GXA_6GXR_01</td>\n",
       "      <td>[\"Modern Fit\"]</td>\n",
       "      <td>[Modern Fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30404</th>\n",
       "      <td>TMW_6NDY_6NDZ_01</td>\n",
       "      <td>Slim Fit</td>\n",
       "      <td>[Slim Fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>TMW_6NE0_6NE1_01</td>\n",
       "      <td>Slim Fit</td>\n",
       "      <td>[Slim Fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30406</th>\n",
       "      <td>TMW_6NE2_6NE3_14</td>\n",
       "      <td>Slim Fit</td>\n",
       "      <td>[Slim Fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30407</th>\n",
       "      <td>TMW_6NFC_6NFD_02</td>\n",
       "      <td>Slim Fit</td>\n",
       "      <td>[Slim Fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30408</th>\n",
       "      <td>TMW_6NFJ_6NFK_11</td>\n",
       "      <td>Slim Fit</td>\n",
       "      <td>[Slim Fit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1509 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            external_id            value        m_name\n",
       "570    TMW_6NJD_6NJE_01  [\"Classic Fit\"]            []\n",
       "571    TMW_8V9G_8VRG_02      Classic Fit            []\n",
       "572    TMW_8V9G_8VRG_19      Classic Fit            []\n",
       "573    TMW_8V9H_8VRH_19      Classic Fit            []\n",
       "574    TMW_6GXA_6GXR_01   [\"Modern Fit\"]  [Modern Fit]\n",
       "...                 ...              ...           ...\n",
       "30404  TMW_6NDY_6NDZ_01         Slim Fit    [Slim Fit]\n",
       "30405  TMW_6NE0_6NE1_01         Slim Fit    [Slim Fit]\n",
       "30406  TMW_6NE2_6NE3_14         Slim Fit    [Slim Fit]\n",
       "30407  TMW_6NFC_6NFD_02         Slim Fit    [Slim Fit]\n",
       "30408  TMW_6NFJ_6NFK_11         Slim Fit    [Slim Fit]\n",
       "\n",
       "[1509 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit[['external_id','value','m_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "1408\n"
     ]
    }
   ],
   "source": [
    "na=fit[(fit['m_name'].astype(str)=='[]')&(fit['m_long'].astype(str)=='[]')&(fit['m_custom'].astype(str)=='[]')]\n",
    "print(len(na))\n",
    "\n",
    "df=fit[(fit['m_name'].astype(str)!='[]')|(fit['m_long'].astype(str)!='[]')|(fit['m_custom'].astype(str)!='[]')]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slim Fit                        834\n",
       "Modern Fit                      281\n",
       "Classic Fit                     173\n",
       "Skinny Fit                       71\n",
       "Executive Fit                    10\n",
       "Athletic Fit                      9\n",
       "[\"Modern Fit\"]                    8\n",
       "[\"Classic Fit\",\"Modern Fit\"]      7\n",
       "Straight Fit                      6\n",
       "Extreme Slim Fit                  5\n",
       "n/a                               2\n",
       "[\"Classic Fit\"]                   1\n",
       "[\"Classic Fit\",\"Slim Fit\"]        1\n",
       "Name: value, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['value'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim=df[(df['value'].astype(str)=='Slim Fit')]#['m_custom'].explode().value_counts()\n",
    "\n",
    "pat='''(?i)(slim.?fit)|()'''\n",
    "slim['names']=slim['m_name'].apply(lambda x: re_extract(pat,str(x)))\n",
    "slim['longs']=slim['m_long'].apply(lambda x: re_extract(pat,str(x)))\n",
    "slim['customs']=slim['m_custom'].apply(lambda x: re_extract(pat,str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=slim[(slim['names'].astype(str)=='[]')&(slim['longs'].astype(str)=='[]')&(slim['customs'].astype(str)=='[]')]\n",
    "s['Q:fit']='[]'\n",
    "match_slim_fit_wipe=s[['external_id','Q:fit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['value'].astype(str)=='Slim Fit')&(df['m_name'].astype(str)!='slim fit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncurated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "formatted_attribute = 'sustainability'\n",
    "buckets = \"\"\"Office Chairs\"\"\"\n",
    "\n",
    "attribute = formatted_attribute.lower().replace(' ','_').replace('-','_')\n",
    "value='%n/a%'\n",
    "params = {'customer_id': customer_id ,\n",
    "          'attribute': attribute,\n",
    "          'buckets': str(buckets.split('\\t'))[1:-1],\n",
    "          'value':value,\n",
    "          'customer_name':customer_name\n",
    "         }\n",
    "curation_col = f'Q:{attribute}'\n",
    "\n",
    "\n",
    "df = query_from_file(file_name='./query/custom_fields.sql', params=params)\n",
    "print(len(df))\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# send to the folder for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def looks_good(customer, attribute,matches,today): \n",
    "    drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "    matches.to_csv(f'{drive_path}/{client} - {attribute}-{get_df_name(matches)}-{today}-matches.csv',index=False) \n",
    "    \n",
    "looks_good(client, attribute,matchesna,today)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute='fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def looks_good(customer, attribute,matches,today): \n",
    "    drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "    matches.to_csv(f'{drive_path}/{client} - {attribute}-{get_df_name(matches)}-{today}-match_slim_fit_wipe.csv',index=False) \n",
    "    \n",
    "looks_good(client, attribute,match_slim_fit_wipe,today)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
